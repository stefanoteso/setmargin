We would like to thank all reviewers for their feedback. We will buy an
additional page for the camera ready to enrich the related works section and
improve the text where it was previously cut due to space limitations.
Additional replies:

REVIEW_1

% - novelty
%     - certamente le alternative/sets non sono nuovi; la novita' e' la
%       suddivisione dello spazio di ricerca
%     - cercare roba recente che cita Guo/Bonilla/Viappiani e dimostrare che
%       non esiste nulla di piu' sul punto
%     - diversity in IR esiste [ma non ha lo stesso scopo (non dipende da w)] ma
%       per spazio ci siamo concentrati su related work piu' vicini; si potrebbe
%       comprare una pagina ulteriore nel camera ready per spiegare.
% 
% - set search not analyzed thoroughly
%     - il metodo e' giustificato anche con k=2, che funziona pure bene
%     - k>2 e' utile in base al costo cognitivo; per questo abbiamo messo 2
%       grafici (per query e per iteration); qui si vede il vantaggio di k>2
%     - no messaggio definitivo per via del costo cognitivo e del modello utente;
%       si possono esplorare alternative ma non e' il nostro fuoco; o vederlo
%       come recommendation set (di 5 prendi 1), pero' avvantaggerebbe il nostro
%       metodo
% 
% - *
%     - il fatto che i Bayes siano piu' lenti sono comunque lo stato dell'arte:
%       c'e' un tradeoff tra numero di domande e scalare nel bayesiano; noi
%       invece otteniamo le stesse perf ma scaliamo molto oltre.
%     - gli attributi booleani c'e' un misunderstanding: il problema e' ben
%       piu' complesso. spiegare attributi in dettaglio e quanti bits.

First of all, there is a misunderstanding regarding the experiment
with the PC dataset.  The choice space is much larger than perceived
by the reviewer: the binary encoding of the 8 features (7 discrete + 1
real) uses 73 boolean attributes. This corresponds to more than 700K
feasible configurations.

While the idea of using sets of candidates to encourage diversity is
not new, the novelty is in extending the large margin paradigm to
sets. We view this as a significant step forward over previous
approaches using max-margin maximization and heuristic methods for
uncertainty reduction [for example: Gajos 2005].

Recently the focus in preference elicitation shifted a bit towards
sequential decision making and preference-based reinforcement
learning. Our literature search found nothing more appropriate than
the works we compare to. We'd be grateful for pointers to other
relevant references.

The main contribution of the work is that splitting the search space
in the fashion proposed achieves state-of-the-art performance at a
fraction of the computational cost. This is certainly true for
pairwise queries (k=2), where all methods can be compared on a fair
basis. For k>2, it is difficult to precisely define the cognitive
load.  That's why we report both per-query and per-iteration results,
where the latter show faster convergence for k>2.

The fact that Bayesian methods are computationally expensive is not
sufficient to rule them out, as they tend to be more accurate on small
scale problems. What we show is that we are able to get comparable
performance on these problems, while being able to scale much more.



REVIEW_2

% R2
% 
% - citare UTA in OR -> citare UTA quando introduciamo le slacks
% - ha ragione, rephrase (primo qualita', secondo diversita', tutti e due diversita' dei buoni)
% - non normalizzando i pesi (che e' solo una possibilita'), la L1 e' lo standard
%   per sparsificare
% - explain that the linearization trick is used in Boutilier et al. 2006
% - chiarire che il num bool e sum domini; ma i problemi sono tranquilli perche'
%   i w utenti sono semplici e gli XOR restringono il problema
% - why should gamma be crossvalidated? intuition:
%     - il vincolo 2 diversifica
%     - il gamma vuole che gli oggetti siano di buona qualita' che in PE servono
%       perche' l'utente non si incazzi; non e' ovvio quale deve essere il suo
%       contributo percio' crossvalidiamo
%     - chiarire i bump a cosa sono dovuti (non a gamma); tagliato per spazio
% - fix footnote 4 only applies to k=1 (chiarisco che chiariro')
% - explain why 80% sparsity mimics a user (typically cares about 3 or 4 features)
%     - siamo d'accordo che con 3-4 features non serve la sparsita'; peccato
%       che dobbiamo simularlo su poche features (synth) for sake of comparisons
%       perche su PC gli altri metodi non funzionano
%     - la sparsita' non l'abbiamo introdotta per far vedere che il nostro sys
%       va meglio -- l'idea e' che scaliamo meglio sia in sparso che in denso.
%       ma usarla e' ragionevole per modellare l'utente. non ci costa nulla
%       toglierla, e non cambia la sua utilita'. il confronto non e' sulla
%       sparsita' ma sul fatto che gli altri non scalano)
%       (infatti la scalabilita' e' il nostro cavallo di battaglia)



Thanks for the reference, it will be integrated.

Correct, <wi,xi> encourages the quality of the xi's, diversity is
given by constraint 2. We will rephrase the sentence.
 
Fixing the 1-norm is not the only option. In our case, the norm is
variable and sparsity is encouraged through LASSO. This is a standard
approach.

We will clarify that the linearization trick is a classic method in OR, and
refer to the paper by Boutilier.

Our plan for extending the approach real variables is not to binarize
them, but rather use appropriate solvers like MIQP

Gamma controls the importance given to the quality of the candidate,
since we do want to favour queries with 'good' items in term of
utility; in principle gamma=0 could be used.  Since the relation
between utility of items in a set and the information value of the
query is not known beforehand (contrary to the settings investigated
by Viappiani&Boutilier), we include gamma in the cross-validation.

Footnote 4 is only relevant to the case k=1 (as constraint 2 vanishes). Will fix.


The intuition for the sparse utility is that when a large number of
features are available, a real user will likely use only few of
them. We agree with the reviewer that with 3 or 4 features sparsity is
not needed, but these were the domain sizes suitable for the
comparison with Bayesian methods. Please note that the scalability of
our method does not depend on the sparsity assumption, which is rather
introduced to account for the bounded rationality of users in large
decision spaces.



REVIEW_3

% - citeremo e commenteremo, grazie
% - margin must be non-negative, make clearer


Thanks for the references, we will make sure to include them.

The margin must be non-negative, as specified in the text above Eq 1;
we will point it out more clearly.

REVIEW_4

% - BT: forse ci favorisce, ci stiamo solo confrontando con quello che fanno gli
%   altri, preso dai competitors; al contrario, sono i modelli bayesiani che
%   lo sfruttano meglio (o almeno che potrebbero)
% - cognitive load: ha ragione, ma tutti fanno domande a coppie es Guo & Sanner,
%   ci accodiamo. non e' chiaro come formalizzare (chiedere agli psicologi)? e'
%   da investigare. in effetti e' quello che vogliamo fare (trovare query che
%   sono piu' facili per l'utente). ma va oltre lo scopo.
% - scalability: stiamo sempre parlando di interazione con l'utente, quindi siamo
%   lontani dal raggiugnere la complex gestibile da un MILP solver. il PC ad es
%   e' gestibile mentre per l'utente e' complesso (non si va sulle migliaia di
%   var booleane). capire quanti Bools un MILP gestisce (chiedere a Roberto?).
%   la sat del solver sta molto sopra la sat dell'utente.
% - reals: future work di sicuro, infatti l'abbiamo messo li' -- le estensioni
%   sarebbero non-lineari per via di <w,x>, citare ultimo paper sanner. in
%   realta' ongoing su hybrid (alternated o nonlinear).

%- We do not think the BT response model favors our method. First, it was
%  previously used to evaluate the competitors, we merely took it from there.
%  Second, given that Bayesian models are tied to the response model, they are
%  more likely to benefit from the additional constraints that the modelling
%  provides.

We adopt Bradley Terry (BT) as a response model as it is a common
assumption in preference elicitation papers. It is true that
constraint 2 induce items that are "different" and will often be
associated with a low prob of error according to the BT reponse model.
But we do not think this as giving an unfair advantage to our method;
actually it is the Bayesian competitors that exploit BT better, as it
is used as an observation model when performing Bayesian updates.

Mdelling the cognitive load is indeed not easy. We have ongoing work
to explore this issue further, in collaboration with psychologists. In
the paper we just followed the common practice of pairwise queries,
for comparability.

User resources strongly limit the overall complexity of the MILP
problem; MILP solvers are well-equipped to solve problems of this
magnitude.

Indeed linearly dependent variables are not very expressive. We are
very much interested in extending the method to hybrid
Boolean/continuous domains, as hinted to in the Conclusion. We are
currently investigating two directions: alternating between
optimization of Boolean and continuous variables, and leveraging
non-linear solvers.  

Thanks for the reference, we'll make sure to include it.
